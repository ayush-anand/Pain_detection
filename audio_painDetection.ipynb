{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "audio_painDetection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltrK3G19lt6U"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzv_nG8qMJwH",
        "outputId": "ff039657-a7b6-49a8-c4e7-9f827519dff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kly8sAm4W9CN",
        "outputId": "86cd6890-5d4c-48e2-8527-a28602908164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from google.colab.patches import cv2_imshow\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x, *args, **kwargs)\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gates = x.chunk(2, dim = -1)\n",
        "        return x * F.gelu(gates)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult * 2),\n",
        "            GEGLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim * mult, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "# attention\n",
        "\n",
        "def attn(q, k, v):\n",
        "    sim = einsum('b i d, b j d -> b i j', q, k)\n",
        "    attn = sim.softmax(dim = -1)\n",
        "    out = einsum('b i j, b j d -> b i d', attn, v)\n",
        "    return out\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, einops_from, einops_to, **einops_dims):\n",
        "        h = self.heads\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
        "\n",
        "        q *= self.scale\n",
        "\n",
        "        # splice out classification token at index 1\n",
        "        (cls_q, q_), (cls_k, k_), (cls_v, v_) = map(lambda t: (t[:, 0:1], t[:, 1:]), (q, k, v))\n",
        "\n",
        "        # let classification token attend to key / values of all patches across time and space\n",
        "        cls_out = attn(cls_q, k, v)\n",
        "\n",
        "        # rearrange across time or space\n",
        "        q_, k_, v_ = map(lambda t: rearrange(t, f'{einops_from} -> {einops_to}', **einops_dims), (q_, k_, v_))\n",
        "\n",
        "        # expand cls token keys and values across time or space and concat\n",
        "        r = q_.shape[0] // cls_k.shape[0]\n",
        "        cls_k, cls_v = map(lambda t: repeat(t, 'b () d -> (b r) () d', r = r), (cls_k, cls_v))\n",
        "\n",
        "        k_ = torch.cat((cls_k, k_), dim = 1)\n",
        "        v_ = torch.cat((cls_v, v_), dim = 1)\n",
        "\n",
        "        # attention\n",
        "        out = attn(q_, k_, v_)\n",
        "\n",
        "        # merge back time or space\n",
        "        out = rearrange(out, f'{einops_to} -> {einops_from}', **einops_dims)\n",
        "\n",
        "        # concat back the cls token\n",
        "        out = torch.cat((cls_out, out), dim = 1)\n",
        "\n",
        "        # merge back the heads\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
        "\n",
        "        # combine heads out\n",
        "        return self.to_out(out)\n",
        "# main classes\n",
        "class TimeSformeraudio(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        num_frames,\n",
        "        num_classes,\n",
        "        image_size = 224,\n",
        "        patch_size = 16,\n",
        "        channels = 1,\n",
        "        depth = 12,\n",
        "        heads = 8,\n",
        "        dim_head = 64,\n",
        "        attn_dropout = 0.,\n",
        "        ff_dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        num_positions = num_frames * num_patches\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.to_patch_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.pos_emb = nn.Embedding(num_positions + 1, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, dim))\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)), # Time attention\n",
        "                PreNorm(dim, Attention(dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)), # Spatial attention\n",
        "                PreNorm(dim, FeedForward(dim, dropout = ff_dropout)) # Feed Forward\n",
        "            ]))\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, video):\n",
        "        b, f, _, h, w, *_, device, p = *video.shape, video.device, self.patch_size\n",
        "        assert h % p == 0 and w % p == 0, f'height {h} and width {w} of video must be divisible by the patch size {p}'\n",
        "\n",
        "        n = (h // p) * (w // p)\n",
        "        video = rearrange(video, 'b f c (h p1) (w p2) -> b (f h w) (p1 p2 c)', p1 = p, p2 = p)\n",
        "        \n",
        "        tokens = self.to_patch_embedding(video)\n",
        "\n",
        "        cls_token = repeat(self.cls_token, 'n d -> b n d', b = b)\n",
        "        x =  torch.cat((cls_token, tokens), dim = 1)\n",
        "        x += self.pos_emb(torch.arange(x.shape[1], device = device))\n",
        "\n",
        "        for (time_attn, spatial_attn, ff) in self.layers:\n",
        "            x = time_attn(x, 'b (f n) d', '(b n) f d', n = n) + x\n",
        "            x = spatial_attn(x, 'b (f n) d', '(b f) n d', f = f) + x\n",
        "            x = ff(x) + x\n",
        "\n",
        "        cls_token = x[:, 0]\n",
        "        \n",
        "        return self.to_out(cls_token)"
      ],
      "metadata": {
        "id": "AUrBJCoiW86n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import torch.optim as optim\n",
        "import moviepy.editor as mp\n",
        "import scipy.io\n",
        "from scipy.io import wavfile\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "def audiospectogram(pathname):\n",
        "  clip = mp.VideoFileClip(pathname)\n",
        "  print(pathname[:-4])\n",
        "  # Insert Local Audio File Path\n",
        "  audiopath = \"/content/drive/MyDrive/dataset/temp.wav\"\n",
        "  \n",
        "  clip.audio.write_audiofile(audiopath)\n",
        "  sr,x = scipy.io.wavfile.read(audiopath)\n",
        "\n",
        "  ## Parameters: 10ms step, 30ms window\n",
        "  nstep = int(sr * 0.01)\n",
        "  nwin  = int(sr * 0.01)\n",
        "  nfft = nwin\n",
        "\n",
        "  window = np.hamming(nwin)\n",
        "\n",
        "  ## will take windows x[n1:n2].  generate\n",
        "  ## and loop over n2 such that all frames\n",
        "  ## fit within the waveform\n",
        "  nn = range(nwin, len(x), nstep)\n",
        "\n",
        "  X = np.zeros( (len(nn), nfft//2) )\n",
        "\n",
        "  for i,n in enumerate(nn):\n",
        "      xseg = x[n-nwin:n]\n",
        "      z = np.fft.fft(window @ xseg, nfft)\n",
        "      X[i,:] = np.log(np.abs(z[:nfft//2]))\n",
        "\n",
        "\n",
        "  ab = cv2.resize(X,(224,X.shape[0]))\n",
        "\n",
        "  valframe =  ab.shape[0]//224 #number of frames of sound\n",
        "  dim1 = (valframe)*224\n",
        "  # print(dim1, val, ab.shape[0])\n",
        "  Y = (ab[:dim1,0:224].T).reshape(valframe,1,224,224)\n",
        "  os.remove(audiopath)\n",
        "  Y = np.asarray(Y)\n",
        "  return Y\n",
        "def audioextract(path,start,end):\n",
        "  # audios = np.asarray(audios)\n",
        "  audios = []\n",
        "  labels = []\n",
        "  count =0  \n",
        "  for filename in os.listdir(path):\n",
        "    if count>= start and count <end:\n",
        "      # break\n",
        "      specframe = audiospectogram(path + filename)\n",
        "      audios.append(specframe)\n",
        "      if filename[5:9] == 'Pain':\n",
        "        labels.append([1,0])\n",
        "      else:\n",
        "        labels.append([0,1])\n",
        "      \n",
        "    elif count>=end:\n",
        "      break\n",
        "    count+=1\n",
        "  audio = torch.tensor(np.asarray(audios)).float()\n",
        "  print(audio.shape)\n",
        "  del audios\n",
        "  return audio,labels\n"
      ],
      "metadata": {
        "id": "sMGd9zO7W8y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DIM = 224\n",
        "IMAGE_SIZE = 224\n",
        "PATCH_SIZE = 16\n",
        "NUM_CLASSES = 2\n",
        "NUM_FRAMES = 8\n",
        "DEPTH = 12\n",
        "HEADS = 8\n",
        "DIM_HEAD = 64               \n",
        "ATTN_DROPOUT = 0.1\n",
        "FF_DROPOUT = 0.1\n",
        "ITERATIONS = 20\n",
        "model = torch.nn.Sequential(\n",
        "    TimeSformeraudio(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, ff_dropout = FF_DROPOUT),\n",
        "    nn.Softmax(dim=1)\n",
        ")"
      ],
      "metadata": {
        "id": "No9BEUwBW835"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video , audlabel = audioextract('/content/drive/MyDrive/dataset/mix data/',0,98)\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "labels = torch.FloatTensor(audlabel)#[[0.4, 0.6] for i in range(len(video))]) # Add here your own labels\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(15):#ITERATIONS):\n",
        "  y_pred = model(video)\n",
        "\n",
        "  loss = loss_fn(y_pred, torch.tensor(labels))\n",
        "  print(\"#\" + str(t), \" loss:\" + str(loss.item()))\n",
        "  \n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "      for param in model.parameters():\n",
        "          param -= learning_rate * param.grad"
      ],
      "metadata": {
        "id": "_EB-zK3_lleb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}