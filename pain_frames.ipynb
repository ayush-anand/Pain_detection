{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pain_frames.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#start"
      ],
      "metadata": {
        "id": "n6tFKpW9f56L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwZ0NuzmSvml",
        "outputId": "1dc5c1a9-a968-48b5-aabb-348634adbffb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXPCMR8FQFE4",
        "outputId": "9ac814cb-625c-4f90-ba7a-32ff29db5d8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.3.2)\n"
          ]
        }
      ],
      "source": [
        "pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = '/content/drive/My Drive/'\n",
        "DATA_DIR = BASE_DIR + \"data/\"\n",
        "FRAMES_INTERVAL = 10"
      ],
      "metadata": {
        "id": "CRtcA2EBQgSu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from google.colab.patches import cv2_imshow\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x, *args, **kwargs)\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gates = x.chunk(2, dim = -1)\n",
        "        return x * F.gelu(gates)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult * 2),\n",
        "            GEGLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim * mult, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "# attention\n",
        "\n",
        "def attn(q, k, v):\n",
        "    sim = einsum('b i d, b j d -> b i j', q, k)\n",
        "    attn = sim.softmax(dim = -1)\n",
        "    out = einsum('b i j, b j d -> b i d', attn, v)\n",
        "    return out\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, einops_from, einops_to, **einops_dims):\n",
        "        h = self.heads\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
        "\n",
        "        q *= self.scale\n",
        "\n",
        "        # splice out classification token at index 1\n",
        "        (cls_q, q_), (cls_k, k_), (cls_v, v_) = map(lambda t: (t[:, 0:1], t[:, 1:]), (q, k, v))\n",
        "\n",
        "        # let classification token attend to key / values of all patches across time and space\n",
        "        cls_out = attn(cls_q, k, v)\n",
        "\n",
        "        # rearrange across time or space\n",
        "        q_, k_, v_ = map(lambda t: rearrange(t, f'{einops_from} -> {einops_to}', **einops_dims), (q_, k_, v_))\n",
        "\n",
        "        # expand cls token keys and values across time or space and concat\n",
        "        r = q_.shape[0] // cls_k.shape[0]\n",
        "        cls_k, cls_v = map(lambda t: repeat(t, 'b () d -> (b r) () d', r = r), (cls_k, cls_v))\n",
        "\n",
        "        k_ = torch.cat((cls_k, k_), dim = 1)\n",
        "        v_ = torch.cat((cls_v, v_), dim = 1)\n",
        "\n",
        "        # attention\n",
        "        out = attn(q_, k_, v_)\n",
        "\n",
        "        # merge back time or space\n",
        "        out = rearrange(out, f'{einops_to} -> {einops_from}', **einops_dims)\n",
        "\n",
        "        # concat back the cls token\n",
        "        out = torch.cat((cls_out, out), dim = 1)\n",
        "\n",
        "        # merge back the heads\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
        "\n",
        "        # combine heads out\n",
        "        return self.to_out(out)\n",
        "# main classes\n",
        "class TimeSformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        num_frames,\n",
        "        num_classes,\n",
        "        image_size = 224,\n",
        "        patch_size = 16,\n",
        "        channels = 3,\n",
        "        depth = 12,\n",
        "        heads = 8,\n",
        "        dim_head = 64,\n",
        "        attn_dropout = 0.,\n",
        "        ff_dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        num_positions = num_frames * num_patches\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.to_patch_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.pos_emb = nn.Embedding(num_positions + 1, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, dim))\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)), # Time attention\n",
        "                PreNorm(dim, Attention(dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)), # Spatial attention\n",
        "                PreNorm(dim, FeedForward(dim, dropout = ff_dropout)) # Feed Forward\n",
        "            ]))\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, video):\n",
        "        b, f, _, h, w, *_, device, p = *video.shape, video.device, self.patch_size\n",
        "        assert h % p == 0 and w % p == 0, f'height {h} and width {w} of video must be divisible by the patch size {p}'\n",
        "\n",
        "        n = (h // p) * (w // p)\n",
        "        video = rearrange(video, 'b f c (h p1) (w p2) -> b (f h w) (p1 p2 c)', p1 = p, p2 = p)\n",
        "        \n",
        "        tokens = self.to_patch_embedding(video)\n",
        "\n",
        "        cls_token = repeat(self.cls_token, 'n d -> b n d', b = b)\n",
        "        x =  torch.cat((cls_token, tokens), dim = 1)\n",
        "        x += self.pos_emb(torch.arange(x.shape[1], device = device))\n",
        "\n",
        "        for (time_attn, spatial_attn, ff) in self.layers:\n",
        "            x = time_attn(x, 'b (f n) d', '(b n) f d', n = n) + x\n",
        "            x = spatial_attn(x, 'b (f n) d', '(b f) n d', f = f) + x\n",
        "            x = ff(x) + x\n",
        "\n",
        "        cls_token = x[:, 0]\n",
        "        \n",
        "        return self.to_out(cls_token)"
      ],
      "metadata": {
        "id": "FojGBrpKQp7w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "framelabel = []\n",
        "paframes = []\n",
        "# def frameextract():\n",
        "framepath = '/content/drive/MyDrive/processed/'\n",
        "for frame_name in os.listdir(framepath):\n",
        "  # print(frame_name[4:7])\n",
        "  if frame_name[4:7] == 'non':\n",
        "    framelabel.append([0,1])\n",
        "  else:\n",
        "    framelabel.append([1,0])\n",
        "  image = cv2.imread(framepath+frame_name)\n",
        "  # print(image, frame_name)\n",
        "  image = np.transpose(np.asarray(cv2.resize(image, (224,224))), (2, 0, 1))\n",
        "  paframes.append(image)\n",
        "painless = paframes[:50]\n",
        "# video = torch.tensor(np.asarray(paframes).reshape(204,1,3,224,224)).float()"
      ],
      "metadata": {
        "id": "0Nm0BIV1RJbk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video = torch.tensor(np.asarray(painless).reshape(50,1,3,224,224)).float()\n",
        "labelless = framelabel[:50]\n",
        "video.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imypkGJlTUzf",
        "outputId": "263fa762-bb29-42b6-80bf-b0b79f09257a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 1, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(framelabel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhySKaUER1Yy",
        "outputId": "ce9df984-44e5-41e0-ac78-137ea982c5e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "204"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DIM = 224\n",
        "IMAGE_SIZE = 224\n",
        "PATCH_SIZE = 16\n",
        "NUM_CLASSES = 2\n",
        "NUM_FRAMES = 1\n",
        "DEPTH = 12\n",
        "HEADS = 8\n",
        "DIM_HEAD = 64\n",
        "ATTN_DROPOUT = 0.1\n",
        "FF_DROPOUT = 0.1\n",
        "ITERATIONS = 20\n",
        "model = torch.nn.Sequential(\n",
        "    TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, ff_dropout = FF_DROPOUT),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "labels = torch.FloatTensor(labelless)#[[0.4, 0.6] for i in range(len(video))]) # Add here your own labels\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(ITERATIONS):\n",
        "  y_pred = model(torch.FloatTensor(np.asarray(video)))\n",
        "\n",
        "  loss = loss_fn(y_pred, torch.tensor(labels))\n",
        "  print(\"#\" + str(t), \" loss:\" + str(loss.item()))\n",
        "  \n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "      for param in model.parameters():\n",
        "          param -= learning_rate * param.grad\n",
        "\n",
        "pred = model(video) # (batch x classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMcEJfQ3RWBh",
        "outputId": "f3a48b28-7fff-4306-fa09-06b3d3c8992f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0  loss:0.36317938566207886\n",
            "#1  loss:0.3863992989063263\n",
            "#2  loss:0.3552611470222473\n",
            "#3  loss:0.3513811528682709\n",
            "#4  loss:0.3417350649833679\n",
            "#5  loss:0.3430057764053345\n",
            "#6  loss:0.32745009660720825\n",
            "#7  loss:0.3202332556247711\n",
            "#8  loss:0.3221796751022339\n",
            "#9  loss:0.34400302171707153\n",
            "#10  loss:0.32600003480911255\n",
            "#11  loss:0.30187729001045227\n",
            "#12  loss:0.29524773359298706\n",
            "#13  loss:0.29629334807395935\n",
            "#14  loss:0.2759099006652832\n",
            "#15  loss:0.2979535758495331\n",
            "#16  loss:0.29341599345207214\n",
            "#17  loss:0.29895851016044617\n",
            "#18  loss:0.2890051305294037\n",
            "#19  loss:0.2848440110683441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2n part images"
      ],
      "metadata": {
        "id": "K9cXjs0Veqw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIM = 224\n",
        "IMAGE_SIZE = 224\n",
        "PATCH_SIZE = 16\n",
        "NUM_CLASSES = 2\n",
        "NUM_FRAMES = 1\n",
        "DEPTH = 12\n",
        "HEADS = 8\n",
        "DIM_HEAD = 64\n",
        "ATTN_DROPOUT = 0.1\n",
        "FF_DROPOUT = 0.1\n",
        "ITERATIONS = 20\n",
        "PATH = '/content/drive/MyDrive/models/painimg1.pth'\n",
        "model = torch.nn.Sequential(\n",
        "    TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, ff_dropout = FF_DROPOUT),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "# torch.save(model.state_dict(), PATH)\n",
        "chkpnt = torch.load(PATH)\n",
        "model.load_state_dict(chkpnt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYdskTDxYAN6",
        "outputId": "e974b69f-93f7-440b-fdc0-119c1695d8d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# del video\n",
        "video = torch.tensor(np.asarray(paframes[50:100]).reshape(50,1,3,224,224)).float()\n",
        "labelless = framelabel[50:100]\n",
        "video.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjVfuuV1RV-E",
        "outputId": "4a3a4cda-860b-47e4-e5a1-e00676736d17"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 1, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.BCELoss()\n",
        "labels = torch.FloatTensor(labelless)#[[0.4, 0.6] for i in range(len(video))]) # Add here your own labels\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(10):#ITERATIONS):\n",
        "  y_pred = model(video)\n",
        "\n",
        "  loss = loss_fn(y_pred, torch.tensor(labels))\n",
        "  print(\"#\" + str(t), \" loss:\" + str(loss.item()))\n",
        "  \n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "      for param in model.parameters():\n",
        "          param -= learning_rate * param.grad\n",
        "\n",
        "pred = model(video) # (batch x classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gk-FPFwkRV7F",
        "outputId": "d0249d22-9ad9-4f5c-e0f4-53d57bdcd22d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0  loss:1.0097123384475708\n",
            "#1  loss:1.057417869567871\n",
            "#2  loss:0.9792995452880859\n",
            "#3  loss:0.98966383934021\n",
            "#4  loss:0.9770166873931885\n",
            "#5  loss:0.9365754723548889\n",
            "#6  loss:0.9193651676177979\n",
            "#7  loss:0.8632614016532898\n",
            "#8  loss:0.8908348679542542\n",
            "#9  loss:0.8753108382225037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/models/painimg2.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "Dveq3WAYRV4H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3rd part images"
      ],
      "metadata": {
        "id": "xdMQAIV5evQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIM = 224\n",
        "IMAGE_SIZE = 224\n",
        "PATCH_SIZE = 16\n",
        "NUM_CLASSES = 2\n",
        "NUM_FRAMES = 1\n",
        "DEPTH = 12\n",
        "HEADS = 8\n",
        "DIM_HEAD = 64\n",
        "ATTN_DROPOUT = 0.1\n",
        "FF_DROPOUT = 0.1\n",
        "ITERATIONS = 20\n",
        "PATH = '/content/drive/MyDrive/models/painimg2.pth'\n",
        "model = torch.nn.Sequential(\n",
        "    TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, ff_dropout = FF_DROPOUT),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "# torch.save(model.state_dict(), PATH)\n",
        "chkpnt = torch.load(PATH)\n",
        "model.load_state_dict(chkpnt)\n",
        "#3rd batch of frames\n",
        "video = torch.tensor(np.asarray(paframes[100:150]).reshape(50,1,3,224,224)).float()\n",
        "labelless = framelabel[100:150]\n",
        "video.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9VpWe2IRV1B",
        "outputId": "da9d1b25-cce7-4842-ee7d-e5424ce512e8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 1, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.BCELoss()\n",
        "labels = torch.FloatTensor(labelless)#[[0.4, 0.6] for i in range(len(video))]) # Add here your own labels\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(15):#ITERATIONS):\n",
        "  y_pred = model(video)\n",
        "\n",
        "  loss = loss_fn(y_pred, torch.tensor(labels))\n",
        "  print(\"#\" + str(t), \" loss:\" + str(loss.item()))\n",
        "  \n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "      for param in model.parameters():\n",
        "          param -= learning_rate * param.grad\n",
        "\n",
        "pred = model(video) # (batch x classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzwwA6lRVyi",
        "outputId": "7c29e95a-680e-45d7-c353-e0c5e5810f6c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0  loss:0.4867725670337677\n",
            "#1  loss:0.5075350999832153\n",
            "#2  loss:0.5013892650604248\n",
            "#3  loss:0.5014815330505371\n",
            "#4  loss:0.49819305539131165\n",
            "#5  loss:0.498831570148468\n",
            "#6  loss:0.4764912724494934\n",
            "#7  loss:0.5002437233924866\n",
            "#8  loss:0.5031458139419556\n",
            "#9  loss:0.5193483829498291\n",
            "#10  loss:0.49719512462615967\n",
            "#11  loss:0.48009976744651794\n",
            "#12  loss:0.4841354489326477\n",
            "#13  loss:0.5032749772071838\n",
            "#14  loss:0.47818148136138916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/models/painimg3.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "AHKLrW-KRVvW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4th part"
      ],
      "metadata": {
        "id": "F75EOMixgRxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIM = 224\n",
        "IMAGE_SIZE = 224\n",
        "PATCH_SIZE = 16\n",
        "NUM_CLASSES = 2\n",
        "NUM_FRAMES = 1\n",
        "DEPTH = 12\n",
        "HEADS = 8\n",
        "DIM_HEAD = 64\n",
        "ATTN_DROPOUT = 0.1\n",
        "FF_DROPOUT = 0.1\n",
        "ITERATIONS = 20\n",
        "\n",
        "\n",
        "model = torch.nn.Sequential(\n",
        "    TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, ff_dropout = FF_DROPOUT),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "# torch.save(model.state_dict(), PATH)\n",
        "PATH = '/content/drive/MyDrive/models/painimg3.pth'\n",
        "chkpnt = torch.load(PATH)\n",
        "model.load_state_dict(chkpnt)\n",
        "#3rd batch of frames\n",
        "video = torch.tensor(np.asarray(paframes[150:200]).reshape(50,1,3,224,224)).float()\n",
        "labelless = framelabel[150:200]\n",
        "video.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV2ZXUmwgRXO",
        "outputId": "69d01ec2-f2a1-48e3-af0a-0d63506d0286"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 1, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.BCELoss()\n",
        "labels = torch.FloatTensor(labelless)#[[0.4, 0.6] for i in range(len(video))]) # Add here your own labels\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(15):#ITERATIONS):\n",
        "  y_pred = model(video)\n",
        "\n",
        "  loss = loss_fn(y_pred, torch.tensor(labels))\n",
        "  print(\"#\" + str(t), \" loss:\" + str(loss.item()))\n",
        "  \n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "      for param in model.parameters():\n",
        "          param -= learning_rate * param.grad\n",
        "\n",
        "pred = model(video) # (batch x classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNMcUfKwgZLf",
        "outputId": "d6227301-d419-4d28-d48a-b320a221f2e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0  loss:0.6799436211585999\n",
            "#1  loss:0.6803609728813171\n",
            "#2  loss:0.712419867515564\n",
            "#3  loss:0.6961427330970764\n",
            "#4  loss:0.7151193022727966\n",
            "#5  loss:0.6885093450546265\n",
            "#6  loss:0.6862586140632629\n",
            "#7  loss:0.6727443933486938\n",
            "#8  loss:0.6699869632720947\n",
            "#9  loss:0.6946759819984436\n",
            "#10  loss:0.6839250922203064\n",
            "#11  loss:0.6662608981132507\n",
            "#12  loss:0.6803510189056396\n",
            "#13  loss:0.6709305047988892\n",
            "#14  loss:0.6865936517715454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/models/painimg4.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "wI1Cl1uxgZD5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Eval"
      ],
      "metadata": {
        "id": "jHqZKrqUl6LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outGT = np.asarray(labelless)\n",
        "outPRED = np.asarray(torch.Tensor.detach(pred))"
      ],
      "metadata": {
        "id": "b04hKBT2qTEt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outGT = np.asarray(labelless)\n",
        "outPRED = np.asarray(torch.Tensor.detach(pred))\n",
        "from sklearn.metrics import *\n",
        "print('F1: {}'.format(f1_score(outGT, outPRED>.5, average=\"samples\")))\n",
        "print('Precision: {}'.format(precision_score(outGT, outPRED>.5, average=\"samples\")))\n",
        "print('Recall: {}'.format(recall_score(outGT, outPRED >.5, average=\"samples\")))\n",
        "print('Accuracy: {}'.format(accuracy_score(outGT, outPRED>.5)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6-KlmIEl5tM",
        "outputId": "89e0f488-5b75-4ad0-83dc-e933a8e527f9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.64\n",
            "Precision: 0.64\n",
            "Recall: 0.64\n",
            "Accuracy: 0.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(outGT)//2):\n",
        "  print(outGT[i], outPRED[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtft3ZavqwFI",
        "outputId": "70392c62-f243-4e75-dc1d-963873cd4041"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0] [0.22223349 0.77776647]\n",
            "[1 0] [0.29166296 0.708337  ]\n",
            "[0 1] [0.22004147 0.7799585 ]\n",
            "[1 0] [0.27216145 0.7278385 ]\n",
            "[0 1] [0.32832697 0.67167306]\n",
            "[1 0] [0.28189862 0.71810144]\n",
            "[0 1] [0.33535546 0.66464454]\n",
            "[1 0] [0.30840933 0.6915907 ]\n",
            "[0 1] [0.29247585 0.70752424]\n",
            "[0 1] [0.2254198  0.77458024]\n",
            "[0 1] [0.24262531 0.7573747 ]\n",
            "[0 1] [0.252408   0.74759203]\n",
            "[1 0] [0.3281416 0.6718584]\n",
            "[0 1] [0.29749554 0.7025044 ]\n",
            "[0 1] [0.3383802  0.66161984]\n",
            "[0 1] [0.3041264  0.69587356]\n",
            "[0 1] [0.28226125 0.7177388 ]\n",
            "[0 1] [0.29282227 0.7071778 ]\n",
            "[0 1] [0.36710197 0.63289803]\n",
            "[1 0] [0.30377892 0.69622105]\n",
            "[0 1] [0.37927523 0.6207248 ]\n",
            "[0 1] [0.26974627 0.73025376]\n",
            "[0 1] [0.37568036 0.6243197 ]\n",
            "[0 1] [0.30206382 0.6979362 ]\n",
            "[1 0] [0.30575326 0.69424677]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "53roQG9AyRKD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}