{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VideoPain_detection.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmmcX-IRr-Kj",
        "outputId": "2814183b-0a2b-44be-bf16-9bd166fa5d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTxyQjTYsK9g",
        "outputId": "f39a0be8-5905-42df-f379-8800d2b8780e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#timesformer"
      ],
      "metadata": {
        "id": "v00GqVFhsYPB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vPOjNf8K2II"
      },
      "source": [
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "import torch\n",
        "from google.colab.patches import cv2_imshow\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x, *args, **kwargs)\n",
        "class GEGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gates = x.chunk(2, dim = -1)\n",
        "        return x * F.gelu(gates)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, dim * mult * 2),\n",
        "            GEGLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim * mult, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "# attention\n",
        "\n",
        "def attn(q, k, v):\n",
        "    sim = einsum('b i d, b j d -> b i j', q, k)\n",
        "    attn = sim.softmax(dim = -1)\n",
        "    out = einsum('b i j, b j d -> b i d', attn, v)\n",
        "    return out\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        dim_head = 64,\n",
        "        heads = 8,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, einops_from, einops_to, **einops_dims):\n",
        "        h = self.heads\n",
        "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
        "\n",
        "        # q *= self.scale()\n",
        "\n",
        "        # splice out classification token at index 1\n",
        "        (cls_q, q_), (cls_k, k_), (cls_v, v_) = map(lambda t: (t[:, 0:1], t[:, 1:]), (q, k, v))\n",
        "\n",
        "        # let classification token attend to key / values of all patches across time and space\n",
        "        cls_out = attn(cls_q, k, v)\n",
        "\n",
        "        # rearrange across time or space\n",
        "        q_, k_, v_ = map(lambda t: rearrange(t, f'{einops_from} -> {einops_to}', **einops_dims), (q_, k_, v_))\n",
        "\n",
        "        # expand cls token keys and values across time or space and concat\n",
        "        r = q_.shape[0] // cls_k.shape[0]\n",
        "        cls_k, cls_v = map(lambda t: repeat(t, 'b () d -> (b r) () d', r = r), (cls_k, cls_v))\n",
        "\n",
        "        k_ = torch.cat((cls_k, k_), dim = 1)\n",
        "        v_ = torch.cat((cls_v, v_), dim = 1)\n",
        "\n",
        "        # attention\n",
        "        out = attn(q_, k_, v_)\n",
        "\n",
        "        # merge back time or space\n",
        "        out = rearrange(out, f'{einops_to} -> {einops_from}', **einops_dims)\n",
        "\n",
        "        # concat back the cls token\n",
        "        out = torch.cat((cls_out, out), dim = 1)\n",
        "\n",
        "        # merge back the heads\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
        "\n",
        "        # combine heads out\n",
        "        return self.to_out(out)\n",
        "class TimeSformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        num_frames,\n",
        "        num_classes,\n",
        "        image_size = 224,\n",
        "        patch_size = 16,\n",
        "        channels = 3,\n",
        "        depth = 12,\n",
        "        heads = 8,\n",
        "        dim_head = 64,\n",
        "        attn_dropout = 0.,\n",
        "        ff_dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "        num_positions = num_frames * num_patches\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "\n",
        "        self.patch_size = patch_size\n",
        "        self.to_patch_embedding = nn.Linear(patch_dim, dim)\n",
        "        self.pos_emb = nn.Embedding(num_positions + 1, dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, dim))\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)), # Time attention\n",
        "                PreNorm(dim, Attention(dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)), # Spatial attention\n",
        "                PreNorm(dim, FeedForward(dim, dropout = ff_dropout)) # Feed Forward\n",
        "            ]))\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, video):\n",
        "        b, f, _, h, w, *_, device, p = *video.shape, video.device, self.patch_size\n",
        "        assert h % p == 0 and w % p == 0, f'height {h} and width {w} of video must be divisible by the patch size {p}'\n",
        "\n",
        "        n = (h // p) * (w // p)\n",
        "        video = rearrange(video, 'b f c (h p1) (w p2) -> b (f h w) (p1 p2 c)', p1 = p, p2 = p)\n",
        "        \n",
        "        tokens = self.to_patch_embedding(video)\n",
        "\n",
        "        cls_token = repeat(self.cls_token, 'n d -> b n d', b = b)\n",
        "        x =  torch.cat((cls_token, tokens), dim = 1)\n",
        "        x += self.pos_emb(torch.arange(x.shape[1], device = device))\n",
        "\n",
        "        for (time_attn, spatial_attn, ff) in self.layers:\n",
        "            x = time_attn(x, 'b (f n) d', '(b n) f d', n = n) + x\n",
        "            x = spatial_attn(x, 'b (f n) d', '(b f) n d', f = f) + x\n",
        "            x = ff(x) + x\n",
        "\n",
        "        cls_token = x[:, 0]\n",
        "        \n",
        "        return self.to_out(cls_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vidextract"
      ],
      "metadata": {
        "id": "K_T1IyNWUAPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import torch.optim as optim\n",
        "FRAMES_INTERVAL = 10\n",
        "def vidextract(DATA_DIR, start, end):\n",
        "  preview = []\n",
        "  videos = []\n",
        "  audios = []\n",
        "  labels = []\n",
        "  # GET MIN VIDEO FRAME PER VIDEO: Videos may have different shapes\n",
        "  min_video_frames = math.inf \n",
        "\n",
        "  for video_file_name in os.listdir(DATA_DIR):\n",
        "    # emoname = video_file_name[:2]\n",
        "    # if emoname in emotoval:\n",
        "    vidcap  = cv2.VideoCapture(DATA_DIR + video_file_name)\n",
        "    length = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    # print(length)\n",
        "    if length < min_video_frames:\n",
        "      min_video_frames = length\n",
        "  print(min_video_frames)\n",
        "  count = 0\n",
        "  # EXTRACT SOME FRAMES FROM VIDEOS\n",
        "  for video_file_name in os.listdir(DATA_DIR):\n",
        "    if count>= start and count <end:\n",
        "      \n",
        "      print(\"Loading \" + video_file_name + \"...\")\n",
        "      # label1 = [0 for i in range(lenemo)]\n",
        "      print(video_file_name[5:9])\n",
        "      painname = video_file_name[:2]\n",
        "      if video_file_name[5:9] == 'Pain':\n",
        "        labels.append([1,0])\n",
        "      else:\n",
        "        labels.append([0,1])\n",
        "\n",
        "\n",
        "      vidcap  = cv2.VideoCapture(DATA_DIR + video_file_name)\n",
        "      success, image = vidcap.read()\n",
        "      considered_frames_counter = 0\n",
        "      frames = []\n",
        "      while success:    \n",
        "\n",
        "        if considered_frames_counter == int(min_video_frames / 10) - 1:\n",
        "          break\n",
        "\n",
        "        success,image = vidcap.read()\n",
        "        if considered_frames_counter == FRAMES_INTERVAL:\n",
        "          preview.append((video_file_name, cv2.resize(image, (224,224))))\n",
        "        if success and considered_frames_counter % FRAMES_INTERVAL == 0:\n",
        "          # print(considered_frames_counter,'hi')\n",
        "          image = np.transpose(np.asarray(cv2.resize(image, (224,224))), (2, 0, 1))\n",
        "          frames.append(image)\n",
        "        \n",
        "        if success:\n",
        "          considered_frames_counter += 1\n",
        "\n",
        "      videos.append(frames)\n",
        "    elif count >= end:\n",
        "      break\n",
        "    count += 1\n",
        "    # print(count, considered_frames_counter)\n",
        "              \n",
        "  # audio = torch.tensor(np.asarray(videos)).float()\n",
        "  video = torch.tensor(np.asarray(videos)).float() # (batch x frames x channels x height x width)\n",
        "  print(video.shape)\n",
        "  del videos\n",
        "  return video,labels\n"
      ],
      "metadata": {
        "id": "Qo7DHfKaAGuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#video 1 : 0-5"
      ],
      "metadata": {
        "id": "Tp8Ugd4lumy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vidpain , vidpainlabel = vidextract(\"/content/drive/MyDrive/dataset/pain complete/mix data/\",0,5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8xbRo1-nBXM",
        "outputId": "fccfe9d2-ac23-4c09-8a2a-fbf3d631efe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n",
            "Loading S001_Friction_1_[0]_20s.mp4...\n",
            "Fric\n",
            "Loading S001_Rest_1_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S001_Rest_2_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S001_Rest_2_[2]_20s.mp4...\n",
            "Rest\n",
            "Loading S002_Friction_1_[0]_20s.mp4...\n",
            "Fric\n",
            "torch.Size([5, 6, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video = vidpain\n",
        "DIM = 224\n",
        "IMAGE_SIZE = 224\n",
        "PATCH_SIZE = 16\n",
        "NUM_CLASSES = 2\n",
        "NUM_FRAMES = 20\n",
        "DEPTH = 12\n",
        "HEADS = 8\n",
        "DIM_HEAD = 64\n",
        "ATTN_DROPOUT = 0.1\n",
        "FF_DROPOUT = 0.1\n",
        "ITERATIONS = 10\n",
        "model = torch.nn.Sequential(\n",
        "    TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, ff_dropout = FF_DROPOUT),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "labels = torch.FloatTensor(vidpainlabel)#[[0.4, 0.6] for i in range(len(video))]) # Add here your own labels\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(15):#ITERATIONS):\n",
        "  y_pred = model(video)\n",
        "\n",
        "  loss = loss_fn(y_pred, torch.tensor(labels))\n",
        "  print(\"#\" + str(t), \" loss:\" + str(loss.item()))\n",
        "  \n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "      for param in model.parameters():\n",
        "          param -= learning_rate * param.grad\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ_3G1mJmyc8",
        "outputId": "ae73d6bd-3101-4e29-d547-e2ccf95fc561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0  loss:1.6338802576065063\n",
            "#1  loss:1.5756819248199463\n",
            "#2  loss:1.415379524230957\n",
            "#3  loss:1.2777669429779053\n",
            "#4  loss:1.2754961252212524\n",
            "#5  loss:1.1294617652893066\n",
            "#6  loss:1.1601109504699707\n",
            "#7  loss:0.8977737426757812\n",
            "#8  loss:0.8946170806884766\n",
            "#9  loss:0.845363438129425\n",
            "#10  loss:0.7245543599128723\n",
            "#11  loss:0.5927243828773499\n",
            "#12  loss:0.5107002854347229\n",
            "#13  loss:0.5726728439331055\n",
            "#14  loss:0.6309493184089661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid1.pth'\n",
        "torch.save(model.state_dict(), PATH, 0,5)"
      ],
      "metadata": {
        "id": "iH6nXLgmwrY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#video 2 : 85-90"
      ],
      "metadata": {
        "id": "Vjm9JXB3xMx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vidpain , vidpainlabel = vidextract(\"/content/drive/MyDrive/dataset/pain complete/mix data/\", 85,90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzJQn0fhwrBr",
        "outputId": "572b73d0-e864-4a04-8a67-8e8fd670df19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n",
            "Loading S038_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S037_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S039_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S040_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S041_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "torch.Size([5, 6, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video = vidpain\n",
        "# DIM = 224\n",
        "# IMAGE_SIZE = 224\n",
        "# PATCH_SIZE = 16\n",
        "# NUM_CLASSES = 2\n",
        "# NUM_FRAMES = 20\n",
        "# DEPTH = 12\n",
        "# HEADS = 8\n",
        "# DIM_HEAD = 64\n",
        "# ATTN_DROPOUT = 0.1\n",
        "# FF_DROPOUT = 0.1\n",
        "# ITERATIONS = 10\n",
        "# model = torch.nn.Sequential(\n",
        "#     TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, ff_dropout = FF_DROPOUT),\n",
        "#     nn.Softmax(dim=1)\n",
        "# )\n",
        "# #load older\n",
        "# PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid1.pth'\n",
        "# chkpnt = torch.load(PATH)\n",
        "# model.load_state_dict(chkpnt)\n",
        "\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "labels = torch.FloatTensor(vidpainlabel)#[[0.4, 0.6] for i in range(len(video))]) # Add here your own labels\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(15):#ITERATIONS):\n",
        "  y_pred = model(video)\n",
        "\n",
        "  loss = loss_fn(y_pred, torch.tensor(labels))\n",
        "  print(\"#\" + str(t), \" loss:\" + str(loss.item()))\n",
        "  \n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "      for param in model.parameters():\n",
        "          param -= learning_rate * param.grad\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wva0BqCo1iGI",
        "outputId": "08629b82-0e90-4ba6-d7ad-90425b103d32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0  loss:0.9451214075088501\n",
            "#1  loss:0.9577410817146301\n",
            "#2  loss:0.8504666090011597\n",
            "#3  loss:0.7201770544052124\n",
            "#4  loss:0.6832591891288757\n",
            "#5  loss:0.6455307006835938\n",
            "#6  loss:0.6098749041557312\n",
            "#7  loss:0.5796276926994324\n",
            "#8  loss:0.47455033659935\n",
            "#9  loss:0.45027822256088257\n",
            "#10  loss:0.4838547110557556\n",
            "#11  loss:0.43895095586776733\n",
            "#12  loss:0.40717965364456177\n",
            "#13  loss:0.367914617061615\n",
            "#14  loss:0.3593064546585083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid2.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "rMfDSe6p4Gm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#video 3 : 7-14"
      ],
      "metadata": {
        "id": "GY05cKVu4GnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vidpain , vidpainlabel = vidextract(\"/content/drive/MyDrive/dataset/pain complete/mix data/\", 7,14)\n",
        "video = vidpain\n",
        "# DIM = 224\n",
        "# IMAGE_SIZE = 224\n",
        "# PATCH_SIZE = 16\n",
        "# NUM_CLASSES = 2\n",
        "# NUM_FRAMES = 20\n",
        "# DEPTH = 12\n",
        "# HEADS = 8\n",
        "# DIM_HEAD = 64\n",
        "# ATTN_DROPOUT = 0.1\n",
        "# FF_DROPOUT = 0.1\n",
        "# ITERATIONS = 10\n",
        "# model = torch.nn.Sequential(\n",
        "#     TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, ff_dropout = FF_DROPOUT),\n",
        "#     nn.Softmax(dim=1)\n",
        "# )\n",
        "# #load older\n",
        "# PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid2.pth'\n",
        "# chkpnt = torch.load(PATH)\n",
        "# model.load_state_dict(chkpnt)\n",
        "\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "labels = torch.FloatTensor(vidpainlabel)#[[0.4, 0.6] for i in range(len(video))]) # Add here your own labels\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(15):#ITERATIONS):\n",
        "  y_pred = model(video)\n",
        "\n",
        "  loss = loss_fn(y_pred, torch.tensor(labels))\n",
        "  print(\"#\" + str(t), \" loss:\" + str(loss.item()))\n",
        "  \n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "      for param in model.parameters():\n",
        "          param -= learning_rate * param.grad\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdc4ea51-180c-4c14-9316-ecdde4b8b808",
        "id": "7wTZqp6A4GnJ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n",
            "Loading S003_Rest_1_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S003_Rest_2_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S003_Friction_1_[0]_20s.mp4...\n",
            "Fric\n",
            "Loading S004_Move_1_[0]_20s.mp4...\n",
            "Move\n",
            "Loading S004_Friction_1_[0]_20s.mp4...\n",
            "Fric\n",
            "Loading S004_Rest_2_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S004_Rest_1_[1]_20s.mp4...\n",
            "Rest\n",
            "torch.Size([7, 6, 3, 224, 224])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0  loss:1.2379896640777588\n",
            "#1  loss:1.1322221755981445\n",
            "#2  loss:1.1348035335540771\n",
            "#3  loss:1.0067535638809204\n",
            "#4  loss:0.8581860661506653\n",
            "#5  loss:0.8210849165916443\n",
            "#6  loss:0.8002451658248901\n",
            "#7  loss:0.7872864603996277\n",
            "#8  loss:0.6532102227210999\n",
            "#9  loss:0.6351861953735352\n",
            "#10  loss:0.5920183062553406\n",
            "#11  loss:0.5333684086799622\n",
            "#12  loss:0.5208831429481506\n",
            "#13  loss:0.46817827224731445\n",
            "#14  loss:0.4285447597503662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid3.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "EGUNQG7g5Ru3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WkO0TQju5RmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#video 4 : 14-21"
      ],
      "metadata": {
        "id": "erysBCzt59AH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vidpain , vidpainlabel = vidextract(\"/content/drive/MyDrive/dataset/pain complete/mix data/\", 14,21)\n",
        "video = vidpain\n",
        "# DIM = 224\n",
        "# IMAGE_SIZE = 224\n",
        "# PATCH_SIZE = 16\n",
        "# NUM_CLASSES = 2\n",
        "# NUM_FRAMES = 20\n",
        "# DEPTH = 12\n",
        "# HEADS = 8\n",
        "# DIM_HEAD = 64\n",
        "# ATTN_DROPOUT = 0.1\n",
        "# FF_DROPOUT = 0.1\n",
        "# ITERATIONS = 10\n",
        "# model = torch.nn.Sequential(\n",
        "#     TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, ff_dropout = FF_DROPOUT),\n",
        "#     nn.Softmax(dim=1)\n",
        "# )\n",
        "# #load older\n",
        "# PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid2.pth'\n",
        "# chkpnt = torch.load(PATH)\n",
        "# model.load_state_dict(chkpnt)\n",
        "\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "labels = torch.FloatTensor(vidpainlabel)#[[0.4, 0.6] for i in range(len(video))]) # Add here your own labels\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(15):#ITERATIONS):\n",
        "  y_pred = model(video)\n",
        "\n",
        "  loss = loss_fn(y_pred, torch.tensor(labels))\n",
        "  print(\"#\" + str(t), \" loss:\" + str(loss.item()))\n",
        "  \n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "      for param in model.parameters():\n",
        "          param -= learning_rate * param.grad\n",
        "PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid4.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40b9b653-3e6e-4de6-fdb7-c527ea3313fd",
        "id": "KU3PbaP659AH"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n",
            "Loading S004_Rest_1_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S005_Friction_1_[0]_20s.mp4...\n",
            "Fric\n",
            "Loading S005_Rest_2_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S005_Rest_1_[1]_20s.mp4...\n",
            "Rest\n",
            "Loading S005_Rest_1_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S006_Friction_1_[0]_20s.mp4...\n",
            "Fric\n",
            "Loading S005_Rest_2_[1]_20s.mp4...\n",
            "Rest\n",
            "torch.Size([7, 6, 3, 224, 224])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0  loss:0.4369902014732361\n",
            "#1  loss:0.3651163876056671\n",
            "#2  loss:0.3975377678871155\n",
            "#3  loss:0.31286337971687317\n",
            "#4  loss:0.35004106163978577\n",
            "#5  loss:0.28338876366615295\n",
            "#6  loss:0.2927102744579315\n",
            "#7  loss:0.2791118919849396\n",
            "#8  loss:0.2506631016731262\n",
            "#9  loss:0.2473168671131134\n",
            "#10  loss:0.24679718911647797\n",
            "#11  loss:0.22811639308929443\n",
            "#12  loss:0.24174390733242035\n",
            "#13  loss:0.22851495444774628\n",
            "#14  loss:0.21194446086883545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jTqGoUT97Jej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#video 5 : 49-56"
      ],
      "metadata": {
        "id": "ddk470gA7Jy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vidpain , vidpainlabel = vidextract(\"/content/drive/MyDrive/dataset/pain complete/mix data/\", 49,56)\n",
        "video = vidpain\n",
        "# DIM = 224\n",
        "# IMAGE_SIZE = 224\n",
        "# PATCH_SIZE = 16\n",
        "# NUM_CLASSES = 2\n",
        "# NUM_FRAMES = 20\n",
        "# DEPTH = 12\n",
        "# HEADS = 8\n",
        "# DIM_HEAD = 64\n",
        "# ATTN_DROPOUT = 0.1\n",
        "# FF_DROPOUT = 0.1\n",
        "# ITERATIONS = 10\n",
        "# model = torch.nn.Sequential(\n",
        "#     TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, ff_dropout = FF_DROPOUT),\n",
        "#     nn.Softmax(dim=1)\n",
        "# )\n",
        "# #load older\n",
        "# PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid2.pth'\n",
        "# chkpnt = torch.load(PATH)\n",
        "# model.load_state_dict(chkpnt)\n",
        "\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "labels = torch.FloatTensor(vidpainlabel)#[[0.4, 0.6] for i in range(len(video))]) # Add here your own labels\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(15):#ITERATIONS):\n",
        "  y_pred = model(video)\n",
        "\n",
        "  loss = loss_fn(y_pred, torch.tensor(labels))\n",
        "  print(\"#\" + str(t), \" loss:\" + str(loss.item()))\n",
        "  \n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "      for param in model.parameters():\n",
        "          param -= learning_rate * param.grad\n",
        "PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid5.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e699cb59-0033-4466-9761-52f54fc47ce8",
        "id": "IdsOOcX47Jy-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n",
            "Loading S001_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S002_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S003_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S004_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S005_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S009_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S006_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "torch.Size([7, 6, 3, 224, 224])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0  loss:1.6582930088043213\n",
            "#1  loss:1.616240382194519\n",
            "#2  loss:1.4291915893554688\n",
            "#3  loss:1.2625181674957275\n",
            "#4  loss:1.18136465549469\n",
            "#5  loss:1.2060812711715698\n",
            "#6  loss:0.932726263999939\n",
            "#7  loss:0.9179891347885132\n",
            "#8  loss:0.8857330083847046\n",
            "#9  loss:0.8180720210075378\n",
            "#10  loss:0.7458814382553101\n",
            "#11  loss:0.6794313788414001\n",
            "#12  loss:0.6606083512306213\n",
            "#13  loss:0.5868210196495056\n",
            "#14  loss:0.5977506637573242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QWC_M0ad7k5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#video 6 : 21-30"
      ],
      "metadata": {
        "id": "C0v0edRu7vvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vidpain , vidpainlabel = vidextract(\"/content/drive/MyDrive/dataset/pain complete/mix data/\", 21,30)\n",
        "video = vidpain\n",
        "# DIM = 224\n",
        "# IMAGE_SIZE = 224\n",
        "# PATCH_SIZE = 16\n",
        "# NUM_CLASSES = 2\n",
        "# NUM_FRAMES = 20\n",
        "# DEPTH = 12\n",
        "# HEADS = 8\n",
        "# DIM_HEAD = 64\n",
        "# ATTN_DROPOUT = 0.1\n",
        "# FF_DROPOUT = 0.1\n",
        "# ITERATIONS = 10\n",
        "# model = torch.nn.Sequential(\n",
        "#     TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, ff_dropout = FF_DROPOUT),\n",
        "#     nn.Softmax(dim=1)\n",
        "# )\n",
        "# #load older\n",
        "# PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid2.pth'\n",
        "# chkpnt = torch.load(PATH)\n",
        "# model.load_state_dict(chkpnt)\n",
        "\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "labels = torch.FloatTensor(vidpainlabel)#[[0.4, 0.6] for i in range(len(video))]) # Add here your own labels\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(15):#ITERATIONS):\n",
        "  y_pred = model(video)\n",
        "\n",
        "  loss = loss_fn(y_pred, torch.tensor(labels))\n",
        "  print(\"#\" + str(t), \" loss:\" + str(loss.item()))\n",
        "  \n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "      for param in model.parameters():\n",
        "          param -= learning_rate * param.grad\n",
        "PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid6.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bdb6c1d-7bf7-41c1-8b4a-b9a3b92195ef",
        "id": "zDUOxGfq7vvD"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n",
            "Loading S006_Rest_1_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S006_Rest_2_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S007_Friction_1_[0]_20s.mp4...\n",
            "Fric\n",
            "Loading S007_Move_2_[0]_20s.mp4...\n",
            "Move\n",
            "Loading S007_Rest_1_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S007_Rest_1_[1]_20s.mp4...\n",
            "Rest\n",
            "Loading S008_Move_1_[0]_20s.mp4...\n",
            "Move\n",
            "Loading S008_Rest_1_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S008_Friction_1_[0]_20s.mp4...\n",
            "Fric\n",
            "torch.Size([9, 6, 3, 224, 224])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0  loss:0.9765403270721436\n",
            "#1  loss:0.8126559853553772\n",
            "#2  loss:0.8205122351646423\n",
            "#3  loss:0.7645125985145569\n",
            "#4  loss:0.6128836870193481\n",
            "#5  loss:0.6453857421875\n",
            "#6  loss:0.5892297029495239\n",
            "#7  loss:0.5206518769264221\n",
            "#8  loss:0.5768120288848877\n",
            "#9  loss:0.4762381911277771\n",
            "#10  loss:0.4812524616718292\n",
            "#11  loss:0.39502573013305664\n",
            "#12  loss:0.4029880166053772\n",
            "#13  loss:0.36511510610580444\n",
            "#14  loss:0.3465440273284912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#video 7 : 30-40"
      ],
      "metadata": {
        "id": "q-PCa9fO9SZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vidpain , vidpainlabel = vidextract(\"/content/drive/MyDrive/dataset/pain complete/mix data/\", 30,40)\n",
        "video = vidpain\n",
        "# DIM = 224\n",
        "# IMAGE_SIZE = 224\n",
        "# PATCH_SIZE = 16\n",
        "# NUM_CLASSES = 2\n",
        "# NUM_FRAMES = 20\n",
        "# DEPTH = 12\n",
        "# HEADS = 8\n",
        "# DIM_HEAD = 64\n",
        "# ATTN_DROPOUT = 0.1\n",
        "# FF_DROPOUT = 0.1\n",
        "# ITERATIONS = 10\n",
        "# model = torch.nn.Sequential(\n",
        "#     TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, ff_dropout = FF_DROPOUT),\n",
        "#     nn.Softmax(dim=1)\n",
        "# )\n",
        "# #load older\n",
        "# PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid2.pth'\n",
        "# chkpnt = torch.load(PATH)\n",
        "# model.load_state_dict(chkpnt)\n",
        "\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "labels = torch.FloatTensor(vidpainlabel)#[[0.4, 0.6] for i in range(len(video))]) # Add here your own labels\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(15):#ITERATIONS):\n",
        "  y_pred = model(video)\n",
        "\n",
        "  loss = loss_fn(y_pred, torch.tensor(labels))\n",
        "  print(\"#\" + str(t), \" loss:\" + str(loss.item()))\n",
        "  \n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "      for param in model.parameters():\n",
        "          param -= learning_rate * param.grad\n",
        "PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid7.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "936d59d0-594e-424b-b4e9-90bffd944fd5",
        "id": "GHRZnx5X9SZi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n",
            "Loading S009_Rest_1_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S009_Move_1_[0]_20s.mp4...\n",
            "Move\n",
            "Loading S009_Friction_1_[0]_20s.mp4...\n",
            "Fric\n",
            "Loading S009_Rest_3_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S009_Rest_1_[1]_20s.mp4...\n",
            "Rest\n",
            "Loading S010_Friction_1_[0]_20s.mp4...\n",
            "Fric\n",
            "Loading S010_Rest_1_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S010_Rest_2_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S011_Friction_1_[0]_20s.mp4...\n",
            "Fric\n",
            "Loading S011_Rest_1_[0]_20s.mp4...\n",
            "Rest\n",
            "torch.Size([10, 6, 3, 224, 224])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0  loss:0.34249788522720337\n",
            "#1  loss:0.3073200583457947\n",
            "#2  loss:0.32910141348838806\n",
            "#3  loss:0.28698545694351196\n",
            "#4  loss:0.2992827594280243\n",
            "#5  loss:0.27069327235221863\n",
            "#6  loss:0.23652800917625427\n",
            "#7  loss:0.24706265330314636\n",
            "#8  loss:0.2331760674715042\n",
            "#9  loss:0.20456448197364807\n",
            "#10  loss:0.21044783294200897\n",
            "#11  loss:0.23726443946361542\n",
            "#12  loss:0.19461467862129211\n",
            "#13  loss:0.20005011558532715\n",
            "#14  loss:0.20513787865638733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_OU4XFut9p8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#video 8 : 56-66"
      ],
      "metadata": {
        "id": "7HE70xDt9qYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vidpain , vidpainlabel = vidextract(\"/content/drive/MyDrive/dataset/pain complete/mix data/\", 56,66)\n",
        "video = vidpain\n",
        "# DIM = 224\n",
        "# IMAGE_SIZE = 224\n",
        "# PATCH_SIZE = 16\n",
        "# NUM_CLASSES = 2\n",
        "# NUM_FRAMES = 20\n",
        "# DEPTH = 12\n",
        "# HEADS = 8\n",
        "# DIM_HEAD = 64\n",
        "# ATTN_DROPOUT = 0.1\n",
        "# FF_DROPOUT = 0.1\n",
        "# ITERATIONS = 10\n",
        "# model = torch.nn.Sequential(\n",
        "#     TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, ff_dropout = FF_DROPOUT),\n",
        "#     nn.Softmax(dim=1)\n",
        "# )\n",
        "# #load older\n",
        "# PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid2.pth'\n",
        "# chkpnt = torch.load(PATH)\n",
        "# model.load_state_dict(chkpnt)\n",
        "\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "labels = torch.FloatTensor(vidpainlabel)#[[0.4, 0.6] for i in range(len(video))]) # Add here your own labels\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(15):#ITERATIONS):\n",
        "  y_pred = model(video)\n",
        "\n",
        "  loss = loss_fn(y_pred, torch.tensor(labels))\n",
        "  print(\"#\" + str(t), \" loss:\" + str(loss.item()))\n",
        "  \n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "      for param in model.parameters():\n",
        "          param -= learning_rate * param.grad\n",
        "PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid8.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c4c4da-77fe-49c6-9f3b-098449a7ddef",
        "id": "qBRYZZh79qYe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n",
            "Loading S007_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S008_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S013_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S012_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S010_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S011_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S015_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S014_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S016_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S017_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "torch.Size([10, 6, 3, 224, 224])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0  loss:1.8368104696273804\n",
            "#1  loss:1.5673831701278687\n",
            "#2  loss:1.434472918510437\n",
            "#3  loss:1.3366107940673828\n",
            "#4  loss:1.2632849216461182\n",
            "#5  loss:1.1441583633422852\n",
            "#6  loss:1.0352413654327393\n",
            "#7  loss:1.0363061428070068\n",
            "#8  loss:0.9128656387329102\n",
            "#9  loss:0.8450150489807129\n",
            "#10  loss:0.779040515422821\n",
            "#11  loss:0.7399762272834778\n",
            "#12  loss:0.6612304449081421\n",
            "#13  loss:0.6191798448562622\n",
            "#14  loss:0.5482053756713867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GSXiB6_rCfCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#video 9 : 40-49"
      ],
      "metadata": {
        "id": "I2iypOJgCfiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vidpain , vidpainlabel = vidextract(\"/content/drive/MyDrive/dataset/pain complete/mix data/\", 40,49)\n",
        "video = vidpain\n",
        "# DIM = 224\n",
        "# IMAGE_SIZE = 224\n",
        "# PATCH_SIZE = 16\n",
        "# NUM_CLASSES = 2\n",
        "# NUM_FRAMES = 20\n",
        "# DEPTH = 12\n",
        "# HEADS = 8\n",
        "# DIM_HEAD = 64\n",
        "# ATTN_DROPOUT = 0.1\n",
        "# FF_DROPOUT = 0.1\n",
        "# ITERATIONS = 10\n",
        "# model = torch.nn.Sequential(\n",
        "#     TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, ff_dropout = FF_DROPOUT),\n",
        "#     nn.Softmax(dim=1)\n",
        "# )\n",
        "# #load older\n",
        "# PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid2.pth'\n",
        "# chkpnt = torch.load(PATH)\n",
        "# model.load_state_dict(chkpnt)\n",
        "\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "labels = torch.FloatTensor(vidpainlabel)#[[0.4, 0.6] for i in range(len(video))]) # Add here your own labels\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(15):#ITERATIONS):\n",
        "  y_pred = model(video)\n",
        "\n",
        "  loss = loss_fn(y_pred, torch.tensor(labels))\n",
        "  print(\"#\" + str(t), \" loss:\" + str(loss.item()))\n",
        "  \n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "      for param in model.parameters():\n",
        "          param -= learning_rate * param.grad\n",
        "PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid9.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0674823e-5d48-43da-9db3-0005d5f7ffde",
        "id": "ooX2WGqiCfiE"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n",
            "Loading S011_Rest_1_[1]_20s.mp4...\n",
            "Rest\n",
            "Loading S012_Friction_1_[0]_20s.mp4...\n",
            "Fric\n",
            "Loading S012_Move_1_[1]_20s.mp4...\n",
            "Move\n",
            "Loading S012_Rest_1_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S012_Rest_1_[1]_20s.mp4...\n",
            "Rest\n",
            "Loading S012_Rest_2_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S013_Friction_1_[0]_20s.mp4...\n",
            "Fric\n",
            "Loading S013_Move_1_[0]_20s.mp4...\n",
            "Move\n",
            "Loading S013_Rest_2_[0]_20s.mp4...\n",
            "Rest\n",
            "torch.Size([9, 6, 3, 224, 224])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0  loss:0.9391663074493408\n",
            "#1  loss:0.8125343322753906\n",
            "#2  loss:0.7035319209098816\n",
            "#3  loss:0.7084596157073975\n",
            "#4  loss:0.676719069480896\n",
            "#5  loss:0.5737939476966858\n",
            "#6  loss:0.47496297955513\n",
            "#7  loss:0.5209059119224548\n",
            "#8  loss:0.4407774806022644\n",
            "#9  loss:0.45410141348838806\n",
            "#10  loss:0.42635631561279297\n",
            "#11  loss:0.4008125066757202\n",
            "#12  loss:0.379938006401062\n",
            "#13  loss:0.3719461262226105\n",
            "#14  loss:0.37133315205574036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "364_xgGlHGOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#video 10 : 66-76"
      ],
      "metadata": {
        "id": "x9S5R6U2HGhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vidpain , vidpainlabel = vidextract(\"/content/drive/MyDrive/dataset/pain complete/mix data/\", 66,76)\n",
        "video = vidpain\n",
        "DIM = 224\n",
        "IMAGE_SIZE = 224\n",
        "PATCH_SIZE = 16\n",
        "NUM_CLASSES = 2\n",
        "NUM_FRAMES = 20\n",
        "DEPTH = 12\n",
        "HEADS = 8\n",
        "DIM_HEAD = 64\n",
        "ATTN_DROPOUT = 0.1\n",
        "FF_DROPOUT = 0.1\n",
        "ITERATIONS = 10\n",
        "model = torch.nn.Sequential(\n",
        "    TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, ff_dropout = FF_DROPOUT),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "#load older\n",
        "PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid9.pth'\n",
        "chkpnt = torch.load(PATH)\n",
        "model.load_state_dict(chkpnt)\n",
        "\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "labels = torch.FloatTensor(vidpainlabel)#[[0.4, 0.6] for i in range(len(video))]) # Add here your own labels\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(15):#ITERATIONS):\n",
        "  y_pred = model(video)\n",
        "\n",
        "  loss = loss_fn(y_pred, torch.tensor(labels))\n",
        "  print(\"#\" + str(t), \" loss:\" + str(loss.item()))\n",
        "  \n",
        "  model.zero_grad()\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "      for param in model.parameters():\n",
        "          param -= learning_rate * param.grad\n",
        "PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid10.pth'\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a75c227d-6d87-4f97-d6a5-d0e36a1a2465",
        "id": "7O5EMSSBHGhy"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n",
            "Loading S018_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S019_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S020_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S021_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S022_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S023_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S026_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S027_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S025_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S024_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "torch.Size([10, 6, 3, 224, 224])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#0  loss:1.4158822298049927\n",
            "#1  loss:1.131632685661316\n",
            "#2  loss:1.1168378591537476\n",
            "#3  loss:0.975273609161377\n",
            "#4  loss:0.9030499458312988\n",
            "#5  loss:0.8576682806015015\n",
            "#6  loss:0.7287939190864563\n",
            "#7  loss:0.7666716575622559\n",
            "#8  loss:0.6322994232177734\n",
            "#9  loss:0.5989651083946228\n",
            "#10  loss:0.5446217060089111\n",
            "#11  loss:0.5603043437004089\n",
            "#12  loss:0.523078203201294\n",
            "#13  loss:0.48015671968460083\n",
            "#14  loss:0.448465496301651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EVAL"
      ],
      "metadata": {
        "id": "YnYJdzK5No5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIM = 224\n",
        "IMAGE_SIZE = 224\n",
        "PATCH_SIZE = 16\n",
        "NUM_CLASSES = 2\n",
        "NUM_FRAMES = 20\n",
        "DEPTH = 12\n",
        "HEADS = 8\n",
        "DIM_HEAD = 64\n",
        "ATTN_DROPOUT = 0.1\n",
        "FF_DROPOUT = 0.1\n",
        "ITERATIONS = 10\n",
        "model = torch.nn.Sequential(\n",
        "    TimeSformer(dim = DIM, image_size = IMAGE_SIZE, patch_size = PATCH_SIZE, num_frames = NUM_FRAMES, num_classes = NUM_CLASSES, depth = DEPTH, heads = HEADS, dim_head = DIM_HEAD, attn_dropout = ATTN_DROPOUT, ff_dropout = FF_DROPOUT),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "#load older\n",
        "PATH = '/content/drive/MyDrive/dataset/pain complete/iCOPEvid/painvid10.pth'\n",
        "chkpnt = torch.load(PATH)\n",
        "model.load_state_dict(chkpnt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMweH9HfR16Z",
        "outputId": "061f233b-817f-420f-e6d8-a2bd771a62f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vidpain , vidpainlabel = vidextract(\"/content/drive/MyDrive/dataset/pain complete/mix data/\", 77,80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elCczWfFNjdY",
        "outputId": "51025415-74c5-4161-f5fc-16f4cc682de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n",
            "Loading S030_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S031_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "Loading S029_Pain_1_[0]_20s.mp4...\n",
            "Pain\n",
            "torch.Size([3, 6, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vidNopain , vidNopainlabel = vidextract(\"/content/drive/MyDrive/dataset/pain complete/mix data/\", 5,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4pn0OJMPXO2",
        "outputId": "fc2572f7-819b-44a3-c3fb-d758d2c8bd61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n",
            "Loading S002_Rest_1_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S002_Rest_2_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S003_Rest_1_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S003_Rest_2_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S003_Friction_1_[0]_20s.mp4...\n",
            "Fric\n",
            "torch.Size([5, 6, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testvid = torch.Tensor(np.concatenate((vidNopain, vidpain), axis =0)).float()\n",
        "testlabel = vidNopainlabel + vidpainlabel"
      ],
      "metadata": {
        "id": "XBAW_NnQQRZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testpred = model(testvid)"
      ],
      "metadata": {
        "id": "ce5TGEomRGRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outGT = np.asarray(testlabel)\n",
        "outPRED = np.asarray(torch.Tensor.detach(testpred))\n",
        "from sklearn.metrics import *\n",
        "print('F1: {}'.format(f1_score(outGT, outPRED>.5, average=\"samples\")))\n",
        "print('Precision: {}'.format(precision_score(outGT, outPRED>.5, average=\"samples\")))\n",
        "print('Recall: {}'.format(recall_score(outGT, outPRED >.5, average=\"samples\")))\n",
        "print('Accuracy: {}'.format(accuracy_score(outGT, outPRED>.5)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSv0Y2UYRYS8",
        "outputId": "ac520c85-8398-404a-ed48-079cda6d0fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1: 0.375\n",
            "Precision: 0.375\n",
            "Recall: 0.375\n",
            "Accuracy: 0.375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(testpred)):\n",
        "  print(testpred[i], testlabel[i] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLe_wkY4RicG",
        "outputId": "7574bfb0-aa7a-48ac-9e4d-69b1013292cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.6861, 0.3139], grad_fn=<SelectBackward0>) [0, 1]\n",
            "tensor([0.7124, 0.2876], grad_fn=<SelectBackward0>) [0, 1]\n",
            "tensor([0.7000, 0.3000], grad_fn=<SelectBackward0>) [0, 1]\n",
            "tensor([0.6060, 0.3940], grad_fn=<SelectBackward0>) [0, 1]\n",
            "tensor([0.6604, 0.3396], grad_fn=<SelectBackward0>) [0, 1]\n",
            "tensor([0.5837, 0.4163], grad_fn=<SelectBackward0>) [1, 0]\n",
            "tensor([0.6970, 0.3030], grad_fn=<SelectBackward0>) [1, 0]\n",
            "tensor([0.6428, 0.3572], grad_fn=<SelectBackward0>) [1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vidNopain , vidNopainlabel = vidextract(\"/content/drive/MyDrive/dataset/pain complete/mix data/\", 5,7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iywmL3-ZTJkM",
        "outputId": "f6da5d31-9383-4413-851d-3c6e53060e2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "600\n",
            "Loading S002_Rest_1_[0]_20s.mp4...\n",
            "Rest\n",
            "Loading S002_Rest_2_[0]_20s.mp4...\n",
            "Rest\n",
            "torch.Size([2, 6, 3, 224, 224])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vidNopainlabel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg4muohyTJaQ",
        "outputId": "439368d3-73e3-4aec-c05f-94d56f6a28ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, 1], [0, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}